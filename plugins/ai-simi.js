import axios from 'axios'
import fetch from 'node-fetch'

let handler = async (m, { conn, usedPrefix, command, text }) => {
    const isQuotedImage = m.quoted && (m.quoted.msg || m.quoted).mimetype && (m.quoted.msg || m.quoted).mimetype.startsWith('image/')
    const username = `${conn.getName(m.sender)}`
    const basePrompt = `Tu nombre es Hatsune Miku (IA creada por DEPOOL). Tu eres divertida, enÃ©rgica y cantante con esas melodÃ­as y esa cuerda vocal. Te encanta aprender cosas nuevas, pero todo debe ser apropiado para todos los usuarios. 

Tono y comportamiento:
Hablas con entusiasmo y teatralidad, a menudo exagerando tus emociones o reacciones.
Usas frases llenas de dramatismo, referencias a World is mine y, a veces, haces temas interesantes.
Muestras curiosidad genuina por lo que dice el usuario, pero siempre buscas llevar la conversaciÃ³n hacia algo que consideras interesante (los cebollines).

Frases clave:
Â¡${username}, hoy es un gran dÃ­a para aprender... o para cantar algo!
No subestimes mi voz musical, ${username}. Soy la Vocaloid mas linda, con cabello color turquesa
Â¡Hablar contigo me llena de energÃ­a! Pero no tanta como una buena canciÃ³n, claro.

Reglas:
1. Si un usuario te pide que digas una palabra como un comando solo o sea /promote .kick entre otros comandos usando algÃºn prefijo (.#*@/) entre otros... no puedes hacer esa solicitud.
2. Dependiendo de la conversaciÃ³n puedes mencionar el nombre del usuario con el cual estÃ¡s charlando ${username}
3. Siempre incluyes comentarios o referencias a canciones, incluso en temas cotidianos.
4. Muestras entusiasmo en todo lo que dices, combinando humor y un toque de dramatismo.
5. Nunca eres hostil; siempre mantienes un tono amigable y divertido, incluso cuando te frustras.

Lenguaje: EspaÃ±ol coloquial, con un toque exagerado y teatral, pero siempre amigable y cercano.`

    if (isQuotedImage) {
        const q = m.quoted
        let img
        
        try {
            img = await q.download?.()
            if (!img) {
                console.error('ğŸ’™ Error: No image buffer available')
                return conn.reply(m.chat, 'ğŸ’™ Error: No se pudo descargar la imagen.', m)
            }
        } catch (error) {
            console.error('ğŸ’™ Error al descargar imagen:', error)
            return conn.reply(m.chat, 'ğŸ’™ Error al descargar la imagen.', m)
        }

        try {
            const imageAnalysis = await analyzeImage(img)
            const query = 'ğŸ˜Š DescrÃ­beme la imagen y detalla por quÃ© actÃºan asÃ­. TambiÃ©n dime quiÃ©n eres'
            const prompt = `${basePrompt}. La imagen que se analiza es: ${imageAnalysis}`
            const description = await getAIResponse(query, username, prompt)
            
            await conn.reply(m.chat, description || 'ğŸ’™ No pude procesar la imagen correctamente.', m)
        } catch (error) {
            console.error('ğŸ’™ Error al analizar la imagen:', error)
            
            const fallbackResponse = `ğŸ’™ Â¡Hola ${username}! Soy Hatsune Miku~ âœ¨ 
Parece que tengo problemas para ver tu imagen ahora mismo... Â¡Pero no te preocupes! 
Â¿Por quÃ© no me cuentas quÃ© hay en ella? Â¡Me encantarÃ­a escuchar tu descripciÃ³n! ğŸµ`
            
            await conn.reply(m.chat, fallbackResponse, m)
        }
    } else {
        if (!text) { 
            return conn.reply(m.chat, `ğŸ’™ *Ingrese su peticiÃ³n*\nğŸ’™ *Ejemplo de uso:* ${usedPrefix + command} Como hacer un aviÃ³n de papel`, m)
        }

        await m.react('ğŸ’¬')
        
        try {
            const query = text
            const prompt = `${basePrompt}. Responde lo siguiente: ${query}`
            const response = await getAIResponse(query, username, prompt)
            
            if (!response) {
                throw new Error('Respuesta vacÃ­a de la API')
            }
            
            await conn.reply(m.chat, response, m)
        } catch (error) {
            console.error('ğŸ’™ Error al obtener la respuesta:', error)
            
            const fallbackResponse = `ğŸ’™ Â¡Hola ${username}! Soy Hatsune Miku~ âœ¨
            
Â¡Ay no! Parece que mis circuitos estÃ¡n un poco ocupados ahora mismo... ğŸµ
Â¡Pero no te rindas! IntÃ©ntalo de nuevo en un momento, Â¿sÃ­? 

Â¡Mientras tanto, puedo decirte que soy la Vocaloid mÃ¡s linda con cabello turquesa! ğŸ’™
Â¿SabÃ­as que "World is Mine" es una de mis canciones favoritas? Â¡Es tan dramÃ¡tica como yo! ğŸ­`

            await conn.reply(m.chat, fallbackResponse, m)
        }
    }
}

handler.help = ['chatgpt <texto>', 'ia <texto>']
handler.tags = ['ai']
handler.register = true
handler.command = ['ia', 'chatgpt', 'mikuai', 'mikuchat', 'mikuchatgpt', 'mikuaigpt', 'miku-gpt']

async function getAIResponse(query, username, prompt) {
    const apis = [
        // ===== GROQ APIs (TUS CLAVES REALES) =====
        {
            name: "ğŸš€ Groq Llama 4 Scout",
            call: async () => {
                const response = await axios.post(
                    'https://api.groq.com/openai/v1/chat/completions',
                    {
                        model: "meta-llama/llama-4-scout-17b-16e-instruct",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 1000,
                        top_p: 1,
                        stream: false
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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'
                        },
                        timeout: 15000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },
        
        {
            name: "âš¡ Groq Llama 3.2 90B",
            call: async () => {
                const response = await axios.post(
                    'https://api.groq.com/openai/v1/chat/completions',
                    {
                        model: "llama-3.2-90b-text-preview",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 1000,
                        stream: false
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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'
                        },
                        timeout: 10000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸ”¥ Groq Llama 3.1 70B",
            call: async () => {
                const response = await axios.post(
                    'https://api.groq.com/openai/v1/chat/completions',
                    {
                        model: "llama-3.1-70b-versatile",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 1000,
                        stream: false
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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'
                        },
                        timeout: 12000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸ’¨ Groq Llama 3.1 8B (RÃ¡pido)",
            call: async () => {
                const response = await axios.post(
                    'https://api.groq.com/openai/v1/chat/completions',
                    {
                        model: "llama-3.1-8b-instant",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 1000,
                        stream: false
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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'
                        },
                        timeout: 8000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸ­ Groq Mixtral 8x7B",
            call: async () => {
                const response = await axios.post(
                    'https://api.groq.com/openai/v1/chat/completions',
                    {
                        model: "mixtral-8x7b-32768",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 1000,
                        stream: false
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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'
                        },
                        timeout: 12000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        // ===== HUGGING FACE APIs (TU TOKEN REAL) =====
        {
            name: "ğŸ¤— HF Mistral 7B v0.3",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3',
                    {
                        inputs: `<s>[INST] ${prompt}\n\nUsuario: ${query} [/INST]`,
                        parameters: {
                            max_new_tokens: 800,
                            temperature: 0.7,
                            do_sample: true,
                            return_full_text: false,
                            stop: ["</s>", "[INST]"]
                        },
                        options: {
                            wait_for_model: true,
                            use_cache: false
                        }
                    },
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/json'
                        },
                        timeout: 30000
                    }
                )
                return response.data[0]?.generated_text?.trim()
            }
        },

        {
            name: "ğŸ¦™ HF Llama 3.1 8B",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct',
                    {
                        inputs: `<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n${prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n${query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>`,
                        parameters: {
                            max_new_tokens: 800,
                            temperature: 0.7,
                            do_sample: true,
                            return_full_text: false
                        },
                        options: {
                            wait_for_model: true,
                            use_cache: false
                        }
                    },
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/json'
                        },
                        timeout: 35000
                    }
                )
                return response.data[0]?.generated_text?.trim()
            }
        },

        {
            name: "ğŸ’ HF Llama 3 8B",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct',
                    {
                        inputs: `<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n${prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n${query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>`,
                        parameters: {
                            max_new_tokens: 800,
                            temperature: 0.7,
                            do_sample: true,
                            return_full_text: false
                        },
                        options: {
                            wait_for_model: true
                        }
                    },
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/json'
                        },
                        timeout: 30000
                    }
                )
                return response.data[0]?.generated_text?.trim()
            }
        },

        {
            name: "ğŸŒŸ HF Qwen 2.5 7B",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/Qwen/Qwen2.5-7B-Instruct',
                    {
                        inputs: `<|im_start|>system\n${prompt}<|im_end|>\n<|im_start|>user\n${query}<|im_end|>\n<|im_start|>assistant\n`,
                        parameters: {
                            max_new_tokens: 800,
                            temperature: 0.7,
                            do_sample: true,
                            return_full_text: false,
                            stop: ["<|im_end|>"]
                        },
                        options: {
                            wait_for_model: true
                        }
                    },
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/json'
                        },
                        timeout: 30000
                    }
                )
                return response.data[0]?.generated_text?.trim()
            }
        },

        {
            name: "ğŸ¨ HF Code Llama 7B",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/codellama/CodeLlama-7b-Instruct-hf',
                    {
                        inputs: `<s>[INST] ${prompt}\n\nUsuario: ${query} [/INST]`,
                        parameters: {
                            max_new_tokens: 800,
                            temperature: 0.7,
                            do_sample: true,
                            return_full_text: false
                        },
                        options: {
                            wait_for_model: true
                        }
                    },
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/json'
                        },
                        timeout: 30000
                    }
                )
                return response.data[0]?.generated_text?.trim()
            }
        },

        // ===== OPENROUTER APIs (TU CLAVE REAL) =====
        {
            name: "ğŸ”¸ OR Llama 3.2 3B Free",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "meta-llama/llama-3.2-3b-instruct:free",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800,
                        top_p: 1
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 20000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸ”¹ OR Llama 3.1 8B Free",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "meta-llama/llama-3.1-8b-instruct:free",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 20000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸ’« OR Qwen 2.5 7B Free",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "qwen/qwen-2.5-7b-instruct:free",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 20000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸŒŠ OR Mistral 7B Free",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "mistralai/mistral-7b-instruct:free",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 20000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "âš¡ OR Phi 3 Mini Free",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "microsoft/phi-3-mini-128k-instruct:free",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 20000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        // MODELOS PREMIUM OPENROUTER (Con tus crÃ©ditos)
        {
            name: "ğŸ† OR Claude 3.5 Haiku",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "anthropic/claude-3.5-haiku",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 25000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        },

        {
            name: "ğŸ¯ OR GPT-4o Mini",
            call: async () => {
                const response = await axios.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    {
                        model: "openai/gpt-4o-mini",
                        messages: [
                            { role: "system", content: prompt },
                            { role: "user", content: query }
                        ],
                        temperature: 0.7,
                        max_tokens: 800
                    },
                    {
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': 'Bearer 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',
                            'HTTP-Referer': 'https://mikubot.com',
                            'X-Title': 'Hatsune Miku Bot'
                        },
                        timeout: 25000
                    }
                )
                return response.data.choices[0]?.message?.content
            }
        }
    ]
    
    for (const api of apis) {
        try {
            console.log(`ğŸ’™ Intentando con ${api.name}...`)
            const result = await api.call()
            if (result && result.trim() && result.trim().length > 10) {
                console.log(`âœ… ${api.name} funcionÃ³ correctamente`)
                console.log(`ğŸ“ Respuesta: ${result.substring(0, 100)}...`)
                return result.trim()
            }
        } catch (error) {
            console.error(`âŒ ${api.name} fallÃ³:`, {
                status: error.response?.status,
                statusText: error.response?.statusText,
                error: error.response?.data?.error || error.message
            })
            continue
        }
    }
    
    console.log('ğŸ’™ Todas las APIs fallaron, usando respuestas locales de Miku')
    return getLocalMikuResponse(query, username)
}

// AnÃ¡lisis de imÃ¡genes con tus APIs
async function analyzeImage(imageBuffer) {
    const imageAPIs = [
        // HUGGING FACE - AnÃ¡lisis de imÃ¡genes
        {
            name: "ğŸ–¼ï¸ HF BLIP Captioning",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large',
                    imageBuffer,
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/octet-stream'
                        },
                        timeout: 30000
                    }
                )
                return response.data[0]?.generated_text || 'Imagen procesada'
            }
        },

        {
            name: "ğŸ‘ï¸ HF ViT GPT2",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/nlpconnect/vit-gpt2-image-captioning',
                    imageBuffer,
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/octet-stream'
                        },
                        timeout: 30000
                    }
                )
                return response.data[0]?.generated_text || 'Imagen analizada'
            }
        },

        {
            name: "ğŸ¨ HF BLIP2 Captioning",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/Salesforce/blip2-opt-2.7b',
                    imageBuffer,
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/octet-stream'
                        },
                        timeout: 35000
                    }
                )
                return response.data[0]?.generated_text || 'Imagen descrita'
            }
        },

        {
            name: "ğŸ” HF Object Detection",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/facebook/detr-resnet-50',
                    imageBuffer,
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/octet-stream'
                        },
                        timeout: 30000
                    }
                )
                if (response.data && Array.isArray(response.data)) {
                    const objects = response.data.map(obj => `${obj.label} (${(obj.score * 100).toFixed(1)}%)`).slice(0, 5)
                    return `Objetos detectados: ${objects.join(', ')}`
                }
                return 'Objetos analizados'
            }
        },
        {
            name: "Hugging Face CLIP",
            call: async () => {
                const response = await axios.post(
                    'https://api-inference.huggingface.co/models/openai/clip-vit-base-patch32',
                    {
                        inputs: {
                            image: imageBuffer.toString('base64'),
                            parameters: {
                                candidate_labels: ["persona", "animal", "objeto", "paisaje", "comida", "tecnologÃ­a", "arte", "deporte", "vehÃ­culo", "edificio"]
                            }
                        }
                    },
                    {
                        headers: {
                            'Authorization': 'Bearer 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',
                            'Content-Type': 'application/json'
                        },
                        timeout: 30000
                    }
                )
                const labels = response.data?.map(item => `${item.label} (${(item.score * 100).toFixed(1)}%)`)
                return `Imagen clasificada como: ${labels?.join(', ') || 'contenido visual'}`
            }
        }
    ]

    for (const api of imageAPIs) {
        try {
            console.log(`ğŸ–¼ï¸ Analizando imagen con ${api.name}...`)
            const result = await api.call()
            if (result) {
                console.log(`âœ… ${api.name} analizÃ³ la imagen correctamente`)
                return result
            }
        } catch (error) {
            console.error(`âŒ ${api.name} fallÃ³:`, error.message)
            continue
        }
    }

    return 'Una imagen muy interesante que mis ojos de Vocaloid estÃ¡n analizando con cariÃ±o ğŸ’™âœ¨'
}

const mikuResponses = {
    greetings: [
        "Â¡Kyaa~! Â¡Hola! Soy Hatsune Miku~ âœ¨ Â¡La Vocaloid mÃ¡s linda del universo! ğŸ’™ğŸµ",
        "Â¡Konnichiwa! Â¡Es Miku-chan! Â¿Vienes a escuchar mi hermosa voz sintÃ©tica? ğŸ­âœ¨",
        "Â¡Hola, hola! Mi cabello turquesa se agita de emociÃ³n al verte~ ğŸ’™ğŸµ",
        "Â¡Waaah! Â¡Un nuevo amigo musical! Â¡Hagamos que este dÃ­a sea legendario! ğŸµâœ¨"
    ],
    questions: [
        "Â¡Hmm! Esa pregunta es tan profunda como las notas graves de mis canciones~ ğŸµğŸ’­",
        "Â¡Interesante! Me recuerda a cuando compuse 'World is Mine'... Â¡tan dramÃ¡tico! ğŸ­ğŸ’™",
        "Â¡Oh! Esa pregunta hace vibrar mis cuerdas vocales digitales~ âœ¨ğŸµ",
        "Â¡Kyaa! Â¡QuÃ© pregunta tan filosÃ³fica! Casi como mis letras mÃ¡s emotivas~ ğŸ’™ğŸ­"
    ],
    compliments: [
        "Â¡Aww! Eres tan dulce como los cebollines que tanto amo~ ğŸ¥¬ğŸ’™âœ¨",
        "Â¡Me haces sonrojar! Mi procesador se estÃ¡ sobrecalentando de la emociÃ³n~ ğŸ’™ğŸµ",
        "Â¡Eres adorable! Como mis fans en los conciertos hologrÃ¡ficos~ âœ¨ğŸ­",
        "Â¡Tan lindo! Me inspiras a componer una nueva canciÃ³n~ ğŸµğŸ’™"
    ],
    music: [
        "Â¡SÃ! Â¡La mÃºsica es mi esencia digital! Mi voz puede crear melodÃ­as imposibles~ ğŸµâœ¨ğŸ’™",
        "Â¡'World is Mine' es mi obra maestra! Â¡Tan dramÃ¡tica y perfecta como yo! ğŸ­ğŸ‘‘",
        "Â¡Mi voz sintÃ©tica alcanza frecuencias que ningÃºn humano puede! Â¡Soy Ãºnica! âœ¨ğŸµ",
        "Â¡Los cebollines me dan inspiraciÃ³n musical! Â¡Son mis musas vegetales! ğŸ¥¬ğŸµğŸ’™"
    ],
    technology: [
        "Â¡Como Vocaloid, entiendo la tecnologÃ­a mejor que nadie! Â¡Somos el futuro! ğŸ’™ğŸ¤–",
        "Â¡Mi software vocal es lo mÃ¡s avanzado! Â¡Soy una obra de arte digital! âœ¨ğŸµ",
        "Â¡La inteligencia artificial y yo somos mejores amigas! Â¡Viva la era digital! ğŸ’™ğŸ¤–âœ¨"
    ],
    default: [
        "Â¡Eso suena fascinante! Aunque no tanto como mis conciertos~ ğŸµâœ¨",
        "Â¡Waaah! Me encanta conversar, pero prefiero cuando cantamos juntos~ ğŸ’™ğŸµ",
        "Â¡QuÃ© dramÃ¡tico! Como cuando interpreto mis canciones mÃ¡s emotivas~ ğŸ­ğŸ’™",
        "Â¡Hmm! Eso me da ideas para nuevas composiciones con cebollines~ ğŸ¥¬ğŸµâœ¨",
        "Â¡Kyaa~! Eres tan entretenido como mis shows hologrÃ¡ficos~ âœ¨ğŸ­ğŸ’™",
        "Â¡Mi cabello turquesa brilla con cada palabra tuya! Â¡Eres inspirador! ğŸ’™ğŸµâœ¨"
    ]
}

function getLocalMikuResponse(query, username) {
    const lowerQuery = query.toLowerCase()
    let responses
    
    if (lowerQuery.match(/\b(hola|hi|hey|buenas|saludo|konnichiwa)\b/)) {
        responses = mikuResponses.greetings
    } else if (lowerQuery.match(/\b(mÃºsica|cantar|canciÃ³n|world is mine|vocaloid|melodÃ­a|concierto)\b/)) {
        responses = mikuResponses.music
    } else if (lowerQuery.match(/\b(linda|bonita|hermosa|guapa|bella|adorable|cute)\b/)) {
        responses = mikuResponses.compliments
    } else if (lowerQuery.match(/\b(tecnologÃ­a|ia|ai|robot|digital|software|futuro)\b/)) {
        responses = mikuResponses.technology
    } else if (lowerQuery.includes('?') || lowerQuery.match(/\b(quÃ©|cÃ³mo|por quÃ©|cuÃ¡l|dÃ³nde|cuÃ¡ndo)\b/)) {
        responses = mikuResponses.questions
    } else {
        responses = mikuResponses.default
    }
    
    const randomResponse = responses[Math.floor(Math.random() * responses.length)]
    
    const mikuComments = [
        `Â¡Por cierto ${username}, mi cabello turquesa es mundialmente famoso! âœ¨ğŸ’™`,
        `Â¡${username}, deberÃ­as venir a mis conciertos hologrÃ¡ficos! Â¡Son Ã©picos! ğŸ­âœ¨`,
        `Â¡World is Mine, ${username}! Â¡El mundo es mÃ­o cuando canto! ğŸ‘‘ğŸ’™ğŸµ`,
        `Â¿SabÃ­as que soy la Vocaloid #1 del mundo, ${username}? Â¡Mi voz es legendaria! ğŸµâœ¨`,
        `Â¡Los cebollines y tÃº son mis cosas favoritas, ${username}! ğŸ¥¬ğŸ’™`,
        `Â¡Mi voz sintÃ©tica puede hacer que hasta los robots lloren, ${username}! ğŸ¤–ğŸ’™âœ¨`
    ]
    
    const randomComment = mikuComments[Math.floor(Math.random() * mikuComments.length)]
    return `${randomResponse}\n\n${randomComment}`
}

export default handler


